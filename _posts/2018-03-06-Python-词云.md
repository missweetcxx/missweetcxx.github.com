---
layout:     post
title:      Python 词云
subtitle:   用Python词云打造一个春
date:       2018-03-06
author:     Sylvia
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Python
    - Practice
    - Tech
---

>“盼望着，盼望着，东风来了，春天的脚步近了”，惊蛰过后的申城已不再春寒料峭，草长莺飞的日子里，再次看到朱自清先生的《春》，恍然中回到年少的课堂，那些春日的时光真是恣意。  
>
>十多年过后再读这篇文章，词藻不算华丽，但每词每句都流露着一个春。今天我就用python将这篇文章中的词语描绘成花朵的样子，愿读这篇文章的你心里也能永住一个春哦。

![《春》的词云](http://upload-images.jianshu.io/upload_images/7208479-bb423c1d36e0b7b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


##### 1. 提取词语
这里使用了[jieba](https://github.com/fxsjy/jieba)，作为一款比较智能的中文分词组件，我认为普通的文章创建词云使用它足够了。  

jieba支持三种分词模式：  

* 精确模式，试图将句子最精确地切开，适合文本分析；

 ```
seg_list = jieba.cut("我来到北京清华大学", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))  
结果：我/ 来到/ 北京/ 清华大学
 ```
 
* 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；

 ```
seg_list = jieba.cut("我来到北京清华大学", cut_all=True)
print("Full Mode: " + "/ ".join(seg_list))  # 全模式
结果：我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学
 ```

* 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

 ```
seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式
print(", ".join(seg_list))
结果：小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造
 ```
 
在这里我用了精准（也就是默认）模式对《春》进行了词语提取，效果还不赖。但是还是有些词提取的不准确，比如“山朗润起来了”分词的结果是“山朗润/起来/了”，显然jieba中没有“朗润”这个词，这里我需要将它手动添加进去：  

```
jieba.add_word('朗润')
seg_list = jieba.cut("山朗润起来了", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))
```
结果变成了“山/朗润/起来/了“，符合预期。

利用jieba，我们将《春》做了分词。但是分词结果中会有很多“无意义”的不希望展示的词，例如：的，都，了，起来等。这里我用了一个txt文档来记录这些无意义的词，并用代码将这些词过滤：  

```
def stopwords_filter(text):
    word_list = []
    with open('stopwords.txt', encoding='utf-8') as f:
        stop_text = f.readlines()
        stop_list = [word.strip() for word in stop_text]
        f.close()
    for word in text:
        if word.strip() not in stop_list:
           word_list.append(word)

    res = ' '.join(word_list)
    return res
```  

进行了过滤的词就都是可以展示的词啦。


##### 2.生成词云
Step 1. 引入WordCloud 

``` 
from wordcloud import WordCloud
``` 
Step 2. 选择遮罩图

```
from scipy.misc import imread
mask = imread("mask.jpg")
```
我选了这张：
![flower.jpg](http://upload-images.jianshu.io/upload_images/7208479-49a3e3228f69f082.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Step 3. 设定词云参数

```
wc = WordCloud(background_color='white',  # 背景色
               max_words=1000,            # 最大显示词数
               colormap='summer',         # 颜色主题
               mask=mask,                 # 词云遮罩
               max_font_size=max_font_size,  # 最大字体大小
               stopwords=STOPWORDS,       # stopwords
               font_path='msyhbd.ttf',    # 字体路径
               random_state=16,           # 排列模式
               )
```
其中，colormap是wordcloud内置的颜色风格，有168种，我比较喜欢的风格有'summer' 、'PiYG\_r'等；这里的STOPWORDS是wordcloud内置的，但对于中文支持并不是很好，所以我们还需要做一次前置的‘过滤’（如上文）；font_path设定了字体的路径，这里我用了雅黑的ttf文件，还不赖；排列模式是指每个字在图中出现的位置，这里可以进行多次尝试选择出最喜欢的排列效果。  

##### 3.显示图片与保存
使用matplotlib中的pyplot来生成图片，代码如下：

```
import matplotlib.pyplot as plt

# wc是上述事例化的WordCloud,text是过滤后的词语列表
wc.generate(text)      

plt.imshow(wc)
# 不显示坐标轴
plt.axis('off')

plt.show(wc)
```
![Set2](http://upload-images.jianshu.io/upload_images/7208479-29ec8218aeb0f969.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![PiYG_r](http://upload-images.jianshu.io/upload_images/7208479-616c0a4130ef5e79.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这里有个小彩蛋，如果不想使用wordcloud内置的颜色主题而是想根据遮罩颜色来生成图片的话，可以使用recolor重新着色，代码如下：

```
from wordcloud import ImageColorGenerator
from scipy.misc import imread

# 解析遮罩
mask_color = imread(mask.jpg)
# 获取遮罩颜色
image_colors = ImageColorGenerator(mask_color)
# 重新着色
plt.imshow(wc.recolor(color_func=image_color))
```
获取的图片是这样的（好吧，遮罩颜色并不如意）：
![使用遮罩颜色](http://upload-images.jianshu.io/upload_images/7208479-02c12f29bfedb366.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

图片获取后可以通过wordcloud自带的保存图片的to_file来将图片保存至相应文件夹： 
 
```
wc.to_file('result_file/wc.png')
```


#####备注：
word_cloud的168种colormap如下：
```
'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap',
'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd',
'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r',
'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd',
'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r',
'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral',
'Spectral_r', 'Vega10', 'Vega10_r', 'Vega20', 'Vega20_r', 'Vega20b', 'Vega20b_r', 'Vega20c', 'Vega20c_r',
'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r',
'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr',
'bwr_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix', 'cubehelix_r',
'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r',
'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg',
'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv',
'hsv_r', 'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'nipy_spectral', 'nipy_spectral_r',
'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r',
'seismic', 'seismic_r', 'spectral', 'spectral_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10',
'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r',
           'viridis', 'viridis_r', 'winter', 'winter_r', 'PuRd_r'
```


